import streamlit as st
import os
from langchain_community.vectorstores import Chroma
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
# RunnablePassthrough, ChatPromptTemplate, StrOutputParserëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough, RunnableParallel
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from typing import List
from dotenv import load_dotenv

# ----------------------------------------------------
# 0. í™˜ê²½ ì„¤ì • (API í‚¤ ì„¤ì •)
# ----------------------------------------------------
load_dotenv()
try:
    # st.secretsì—ì„œ í‚¤ë¥¼ ë¨¼ì € ì°¾ê³ , ì—†ìœ¼ë©´ .envì—ì„œ ì°¾ìŠµë‹ˆë‹¤.
    api_key = st.secrets.get("OPENAI_API_KEY") or os.getenv("OPENAI_API_KEY")
    if not api_key:
         raise ValueError("OpenAI API í‚¤ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    os.environ["OPENAI_API_KEY"] = api_key
except Exception as e:
    st.error(f"API í‚¤ ë¡œë“œ ì˜¤ë¥˜: {e}. .streamlit/secrets.toml ë˜ëŠ” .env íŒŒì¼ì„ í™•ì¸í•˜ì„¸ìš”.")
    st.stop()
    
# ----------------------------------------------------
# 1. ìƒìˆ˜ ë° ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜
# ----------------------------------------------------
CHROMA_PATH = './chromadb/pandas_rst'
EMBEDDING_MODEL = 'text-embedding-3-small'
LLM_MODEL = "gpt-4o-mini"

def format_docs(docs: List[Document]) -> str:
    """ê²€ìƒ‰ëœ LangChain Document ê°ì²´ë“¤ì„ í•˜ë‚˜ì˜ ë¬¸ìì—´ ì»¨í…ìŠ¤íŠ¸ë¡œ ê²°í•©"""
    return "\n\n".join(doc.page_content for doc in docs)

def safe_filename(path: str) -> str:
    """ê²½ë¡œì—ì„œ íŒŒì¼ëª…ë§Œ ì•ˆì „í•˜ê²Œ ì¶”ì¶œ"""
    try:
        return os.path.basename(path)
    except Exception:
        return "unknown"

def map_source_to_label(source: str) -> str:
    """sourceê°€ URLì´ë©´ í´ë¦­ ê°€ëŠ¥í•œ ë§í¬ë¡œ, ì•„ë‹ˆë©´ íŒŒì¼ëª…ë§Œ í‘œì‹œ"""
    if not source:
        return "`unknown`"
    if source.startswith("http"):
        filename = safe_filename(source)
        return f"[{filename}]({source})"
    # ë¡œì»¬ ê²½ë¡œì¼ ê²½ìš°
    filename = safe_filename(source)
    return f"`{filename}`"

# ----------------------------------------------------
# 2. ìºì‹±ì„ í†µí•œ RAG ì»´í¬ë„ŒíŠ¸ ë¡œë“œ/ìƒì„±
# ----------------------------------------------------
@st.cache_resource
def get_vector_store():
    """ChromaDBë¥¼ ë¡œë“œí•˜ê³  OpenAI ì„ë² ë”© í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ë²¡í„° ì €ì¥ì†Œë¥¼ ì¤€ë¹„"""
    try:
        vectorstore = Chroma(
            persist_directory=CHROMA_PATH,
            embedding_function=OpenAIEmbeddings(model=EMBEDDING_MODEL)
        )
        # ChromaDB ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆëŠ”ì§€ ê°„ë‹¨íˆ ê²€ì¦
        if vectorstore._collection.count() == 0:
             st.error("ChromaDB ì»¬ë ‰ì…˜ì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ì„ë² ë”©ì´ í•„ìš”í•©ë‹ˆë‹¤. 'python ì³‡ë´‡_v0.ipynb'ë¥¼ ì‹¤í–‰í–ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.")
             return None
             
        return vectorstore
    except Exception as e:
        st.error(f"ChromaDB ë¡œë“œ ì¤‘ ì‹¬ê°í•œ ì˜¤ë¥˜ ë°œìƒ. ì›ì¸: {type(e).__name__}: {e}")
        return None

def setup_rag_components(_vectorstore: Chroma):
    """LLM, í”„ë¡¬í”„íŠ¸, Retriever ì„¤ì • ë° RAG ì²´ì¸ êµ¬ì¶•"""
    if _vectorstore is None:
        return None

    retriever = _vectorstore.as_retriever(search_kwargs={"k": 4})
    llm = ChatOpenAI(model=LLM_MODEL, temperature=0)

    # ë‹µë³€ ìƒì„± í”„ë¡¬í”„íŠ¸
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system",
         "You are an assistant for question-answering tasks focused on Pandas documentation. "
         "Use the following pieces of retrieved context to answer the question. "
         "If you don't know the answer, just say that you don't know. "
         "Use three sentences maximum and keep the answer concise.\n\n"
         "Context: {context}"),
        ("human", "{question}")
    ])

    # 1. ë¬¸ì„œ ê²€ìƒ‰ ì²´ì¸: ê²€ìƒ‰ ê²°ê³¼ë¥¼ format_docsë¡œ ë¬¸ìì—´í™”
    context_chain = retriever | format_docs
    
    # 2. RAG ì²´ì¸: ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì™€ ì§ˆë¬¸ì„ LLMì— ì „ë‹¬í•˜ì—¬ ë‹µë³€ ìƒì„±
    # RunnableParallelì„ ì‚¬ìš©í•˜ì—¬ ê²€ìƒ‰ ê²°ê³¼ì™€ ë‹µë³€ì„ ëª¨ë‘ ë°˜í™˜í•˜ë„ë¡ ì²´ì¸ êµ¬ì¡°ë¥¼ ë³€ê²½í•©ë‹ˆë‹¤.
    # 'retrieved_docs' í‚¤ë¥¼ í†µí•´ ì›ë³¸ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë³„ë„ë¡œ ì €ì¥í•˜ì—¬ ì¶œì²˜ í‘œì‹œì— ì‚¬ìš©í•©ë‹ˆë‹¤.
    rag_chain = RunnableParallel(
        # 'answer' í‚¤ì— ë‹µë³€ì„ ì €ì¥ (ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ + ì§ˆë¬¸ -> LLM -> ë¬¸ìì—´)
        answer = {
            "context": context_chain, 
            "question": RunnablePassthrough()
        } | qa_prompt | llm | StrOutputParser(),
        
        # 'retrieved_docs' í‚¤ì— ê²€ìƒ‰ëœ ë¬¸ì„œ ê°ì²´ ë¦¬ìŠ¤íŠ¸ë¥¼ ê·¸ëŒ€ë¡œ ì €ì¥ (ì¶œì²˜ í‘œì‹œìš©)
        retrieved_docs = retriever
        
    )
    
    return rag_chain

# ----------------------------------------------------
# 3. Streamlit UI ì •ì˜
# ----------------------------------------------------
st.set_page_config(page_title="Pandas RAG Q&A ğŸ¼", layout="wide")
st.title("ğŸ¼ Pandas ê³µì‹ ë¬¸ì„œ ê¸°ë°˜ RAG ì±—ë´‡")
st.markdown("---")

vectorstore = get_vector_store()
rag_chain = setup_rag_components(vectorstore)


if rag_chain:
    user_question = st.text_input(
        "Pandasì— ëŒ€í•´ ê¶ê¸ˆí•œ ì ì„ ì§ˆë¬¸í•˜ì„¸ìš”:",
        placeholder="ì˜ˆ: DataFrameì—ì„œ ëˆ„ë½ëœ ê°’(NaN)ì„ í™•ì¸í•˜ëŠ” ë©”ì„œë“œëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
    )

    if user_question:
        with st.spinner("Pandas ë¬¸ì„œì—ì„œ ë‹µë³€ì„ ê²€ìƒ‰í•˜ëŠ” ì¤‘..."):
            try:
                # 1. RAG ì²´ì¸ ì‹¤í–‰ (ì´ì œ 'answer'ì™€ 'retrieved_docs' ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜)
                response = rag_chain.invoke(user_question)

                # 2. ë‹µë³€ í‘œì‹œ
                st.subheader("ğŸ¤– ë‹µë³€")
                st.info(response['answer'])

                # 3. ê²€ìƒ‰ëœ ì¶œì²˜ í‘œì‹œ (ì˜¤ë¥˜ ì—†ì´ ë³µêµ¬)
                st.subheader("ğŸ” ê²€ìƒ‰ëœ ì¶œì²˜ (Context)")
                retrieved_docs = response['retrieved_docs'] # ì²´ì¸ ê²°ê³¼ì—ì„œ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜´

                for i, doc in enumerate(retrieved_docs):
                    source_label = map_source_to_label(doc.metadata.get("source", ""))
                    with st.expander(f"ì²­í¬ {i+1} â€¢ ì¶œì²˜: {source_label}"):
                        # doc.page_contentëŠ” ë¬¸ì„œ ì²­í¬ ë‚´ìš©
                        st.code(doc.page_content, language="text")

            except Exception as e:
                st.error(f"RAG ì²´ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {type(e).__name__}: {e}")
else:
    # vectorstore ë˜ëŠ” rag_chainì´ Noneì¸ ê²½ìš° (API í‚¤ ë˜ëŠ” ChromaDB ê²½ë¡œ ë¬¸ì œ)
    st.warning("RAG ì‹œìŠ¤í…œì„ ì´ˆê¸°í™”í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ìƒë‹¨ì˜ ì˜¤ë¥˜ ë©”ì‹œì§€ë‚˜ í„°ë¯¸ë„ì„ í™•ì¸í•˜ì„¸ìš”.")